{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import skimage.io\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count the number of pixels in each affordance\n",
    "# import glob\n",
    "\n",
    "# path_list = glob.glob('./part-affordance-dataset/tools/*', recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path_list_train = []\n",
    "# image_path_list_train_with_label = []\n",
    "# image_path_list_train_without_label = []\n",
    "# image_path_list_test = []\n",
    "\n",
    "# for path in path_list:\n",
    "#     i = glob.glob(path + '/*rgb.jpg')\n",
    "    \n",
    "#     image_path_list_test += i[:50]\n",
    "#     image_path_list_train += i[50:]\n",
    "\n",
    "# for i, path in enumerate(image_path_list_train):\n",
    "#     if i%2 == 0:\n",
    "#         image_path_list_train_with_label.append(path)\n",
    "#     else:\n",
    "#         image_path_list_train_without_label.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_path_list_train_with_label = []\n",
    "# class_path_list_test = []\n",
    "\n",
    "# for path in image_path_list_train_with_label:\n",
    "#     c = path[:-7] + 'label.mat'\n",
    "#     class_path_list_train_with_label.append(c)\n",
    "    \n",
    "# for path in image_path_list_test:\n",
    "#     c = path[:-7] + 'label.mat'\n",
    "#     class_path_list_test.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_with_label = pd.DataFrame({\n",
    "#     \"image_path\": image_path_list_train_with_label,\n",
    "#     \"class_path\": class_path_list_train_with_label},\n",
    "#     columns=[\"image_path\", \"class_path\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_without_label = pd.DataFrame({\n",
    "#     \"image_path\": image_path_list_train_without_label\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.DataFrame({\n",
    "#     \"image_path\": image_path_list_test,\n",
    "#     \"class_path\": class_path_list_test},\n",
    "#     columns=[\"image_path\", \"class_path\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_with_label.to_csv('train_with_label.csv', index=None)\n",
    "# df_train_without_label.to_csv('train_without_label.csv', index=None)\n",
    "# df_test.to_csv('test.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartAffordanceDataset(Dataset):\n",
    "    \"\"\"Part Affordance Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_class_path = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_class_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_class_path.iloc[idx, 0]\n",
    "        class_path = self.image_class_path.iloc[idx, 1]\n",
    "        image = Image.open(image_path) # read as numpy array\n",
    "        cls = scipy.io.loadmat(class_path)[\"gt_label\"]\n",
    "        \n",
    "        sample = {'image': image, 'class': cls}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartAffordanceDatasetWithoutLabel(Dataset):\n",
    "    \"\"\" Part Affordance Dataset without label \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_class_path = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_class_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_class_path.iloc[idx]\n",
    "        image = skimage.io.imread(image_path) # read as numpy array\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_numpy(array, crop_height, crop_weight):\n",
    "    h, w = array.shape\n",
    "    return array[h//2 - crop_height//2: h//2 + crop_height//2,\n",
    "                 w//2 - crop_weight//2: w//2 + crop_weight//2\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_pil_image(pil_img, crop_width, crop_height):\n",
    "    img_width, img_height = pil_img.size\n",
    "    return pil_img.crop(((img_width - crop_width) // 2,\n",
    "                         (img_height - crop_height) // 2,\n",
    "                         (img_width + crop_width) // 2,\n",
    "                         (img_height + crop_height) // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterCrop(object):\n",
    "    def __call__(self, sample):\n",
    "        image, cls = sample['image'], sample['class']\n",
    "        \n",
    "        image = crop_center_pil_image(image, 320, 320)\n",
    "        cls = crop_center_numpy(cls, 320, 320)\n",
    "        \n",
    "        return {'image': image, 'class': cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_interpolate(array, new_size):\n",
    "    ''' nearest neighbor interpolation '''\n",
    "    \n",
    "    new_array = np.zeros(new_size)\n",
    "    old_size = array.shape\n",
    "    ratio = new_size[0] / old_size[0], new_size[1] / old_size[1]\n",
    "    \n",
    "    for i in range(new_size[0]):\n",
    "        for j in range(new_size[1]):\n",
    "            new_array[i][j] = array[int(np.round(i/ratio[0]))][int(np.round(j/ratio[1]))]\n",
    "            \n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    ''' Resize the images and labels '''\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, cls = sample['image'], sample['class']\n",
    "        \n",
    "        image = transforms.functional.resize(image, (224, 224)) # input size of ResNet is (224, 224)\n",
    "        cls = nn_interpolate(cls, (224, 224))\n",
    "        \n",
    "        image = np.asarray(image)\n",
    "        \n",
    "        return {'image': image, 'class': cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, cls = sample['image'], sample['class']\n",
    "        \n",
    "        image = np.asarray(image)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image).float(), \n",
    "                'class': torch.from_numpy(cls).long()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = PartAffordanceDataset('image_class_path.csv',\n",
    "#                                 transform=transforms.Compose([\n",
    "#                                     CenterCrop(),\n",
    "#                                     ToTensor()\n",
    "#                                 ]))\n",
    "\n",
    "# data_loader = DataLoader(data, batch_size=10, shuffle=False)\n",
    "\n",
    "# mean = 0\n",
    "# std = 0\n",
    "# n = 0\n",
    "\n",
    "# for sample in data_loader:\n",
    "#     img = sample['image']   \n",
    "#     img = img.view(len(img), 3, -1)\n",
    "#     mean += img.mean(2).sum(0)\n",
    "#     std += img.std(2).sum(0)\n",
    "#     n += len(img)\n",
    "    \n",
    "# mean /= n\n",
    "# std /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[55.8630, 59.9099, 91.7419]\n",
    "std=[31.6852, 29.8496, 19.0835]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __call__(self, sample):\n",
    "        image, cls = sample['image'], sample['class']\n",
    "        \n",
    "        image = transforms.functional.normalize(image, mean, std)\n",
    "        \n",
    "        return {'image': image, 'class': cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_label = PartAffordanceDataset('train_with_label.csv',\n",
    "                                            transform=transforms.Compose([\n",
    "                                                CenterCrop(),\n",
    "                                                Resize(),\n",
    "                                                ToTensor(),\n",
    "                                                Normalize()\n",
    "                                            ]))\n",
    "\n",
    "train_data_without_label = PartAffordanceDatasetWithoutLabel('train_with_label.csv',\n",
    "                                            transform=transforms.Compose([\n",
    "                                                CenterCrop(),\n",
    "                                                Resize(),\n",
    "                                                ToTensor(),\n",
    "                                                Normalize()\n",
    "                                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = PartAffordanceDataset('test.csv',\n",
    "                                transform=transforms.Compose([\n",
    "                                    CenterCrop(),\n",
    "                                    Resize(),\n",
    "                                    ToTensor(),\n",
    "                                    Normalize()\n",
    "                                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_with_label = DataLoader(train_data_with_label, batch_size=1, shuffle=True)\n",
    "test_loader_without_label = DataLoader(train_data_without_label, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the number of pixels in each class after center crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "\n",
    "class PartAffordanceDataset(Dataset):\n",
    "    \"\"\"Part Affordance Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_class_path = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_class_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_class_path.iloc[idx, 0]\n",
    "        class_path = self.image_class_path.iloc[idx, 1]\n",
    "        image = Image.open(image_path)\n",
    "        cls = scipy.io.loadmat(class_path)[\"gt_label\"]\n",
    "        \n",
    "        sample = {'image': image, 'class': cls}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class PartAffordanceDatasetWithoutLabel(Dataset):\n",
    "    \"\"\" Part Affordance Dataset without label \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_class_path = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_class_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_class_path.iloc[idx]\n",
    "        image = Image.open(image_path) \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image\n",
    "\n",
    "\n",
    "\n",
    "''' transforms for pre-processing '''\n",
    "\n",
    "def crop_center_numpy(array, crop_height, crop_weight):\n",
    "    h, w = array.shape\n",
    "    return array[h//2 - crop_height//2: h//2 + crop_height//2,\n",
    "                 w//2 - crop_weight//2: w//2 + crop_weight//2\n",
    "                ]\n",
    "\n",
    "\n",
    "def crop_center_pil_image(pil_img, crop_height, crop_width):\n",
    "    w, h = pil_img.size\n",
    "    return pil_img.crop(((w - crop_width) // 2,\n",
    "                         (h - crop_height) // 2,\n",
    "                         (w + crop_width) // 2,\n",
    "                         (h + crop_height) // 2))\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __call__(self, sample):\n",
    "        image, cls = sample['image'], sample['class']\n",
    "        \n",
    "        image = crop_center_pil_image(image, 256, 320)\n",
    "        cls = crop_center_numpy(cls, 256, 320)\n",
    "        \n",
    "        return {'image': image, 'class': cls}\n",
    "\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, cls = sample['image'], sample['class']\n",
    "        \n",
    "        return {'image': transforms.functional.to_tensor(image).float(), \n",
    "                'class': torch.from_numpy(cls).long()}\n",
    "\n",
    "\n",
    "\n",
    "mean=[55.8630, 59.9099, 91.7419]\n",
    "std=[31.6852, 29.8496, 19.0835]\n",
    "\n",
    "class Normalize(object):\n",
    "    def __call__(self, sample):\n",
    "        image, cls = sample['image'], sample['class']\n",
    "        \n",
    "        image = transforms.functional.normalize(image, mean, std)\n",
    "        \n",
    "        return {'image': image, 'class': cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PartAffordanceDataset('train_with_label.csv',\n",
    "                                transform=transforms.Compose([\n",
    "                                    CenterCrop(),\n",
    "                                    ToTensor()\n",
    "                                ]))\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "cnt_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}\n",
    "\n",
    "for sample in data_loader:\n",
    "    img = sample['class'].numpy()\n",
    "    \n",
    "    num, cnt = np.unique(img, return_counts=True)\n",
    "    \n",
    "    for n, c in zip(num, cnt):\n",
    "        cnt_dict[n] += c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{0: 1151953630,  \n",
    " 1: 14085528,  \n",
    " 2: 6604904,  \n",
    " 3: 5083312,  \n",
    " 4: 15579160,  \n",
    " 5: 2786632,  \n",
    " 6: 3814170,  \n",
    " 7: 8105464}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = torch.tensor([1151953630, 14085528, 6604904, 5083312,\n",
    "                        15579160, 2786632, 3814170, 8105464])\n",
    "total = class_num.sum().item()\n",
    "frequency = class_num.float() / total\n",
    "median = torch.median(frequency)\n",
    "class_weight = median / frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0057, 0.4689, 1.0000, 1.2993, 0.4240, 2.3702, 1.7317, 0.8149])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    \"\"\" Atrous Spatial Pyramid Pooling \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel, pyramids):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.atrous_convs = nn.Module()\n",
    "        \n",
    "        for i, p in enumerate(pyramids):\n",
    "            self.atrous_convs.add_module(\n",
    "                \"atrous_conv{}\".format(i),\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size=3, stride=1, padding=p, dilation=p, bias=True)\n",
    "            )\n",
    "        \n",
    "        for m in self.atrous_convs.children():\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        h = 0\n",
    "        for atrous_conv in self.atrous_convs.children():\n",
    "            h += atrous_conv(x)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabV2(nn.Module):\n",
    "    ''' ResNet => ASPP '''\n",
    "    \n",
    "    def __init__(self, in_channel, n_classes, pyramids):\n",
    "        super().__init__()\n",
    "        \n",
    "        resnet = torchvision.models.resnet101(pretrained=True)\n",
    "        \n",
    "        for m in resnet.layer2.children():\n",
    "            m.conv1.stride = (2, 2)\n",
    "            break\n",
    "        \n",
    "        for m in resnet.layer3.children():\n",
    "            m.conv2.dilation = (2, 2)\n",
    "        \n",
    "        for m in resnet.layer4.children():\n",
    "            m.conv2.dilation = (4, 4)\n",
    "        \n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.aspp = ASPP(2048, n_classes, pyramids)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.aspp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLabV2(3, 8, [6, 12, 18, 24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3, 3, 240, 240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (15) must match the existing size (30) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-381bbdcc833e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-6243b4880ff6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maspp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch/lib/python3.5/site-packages/torchvision-0.2.1-py3.5.egg/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (15) must match the existing size (30) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 8, 10])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = nn.functional.interpolate(y, (240, 320), mode='bilinear', align_corners=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size([3, 21, 31, 41])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class_weigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = torch.tensor([2078085712, 34078992, 15921090, 12433420, \n",
    "                          38473752, 6773528, 9273826, 20102080])\n",
    "\n",
    "total = class_num.sum().item()\n",
    "\n",
    "frequency = class_num.float() / total\n",
    "median = torch.median(frequency)\n",
    "\n",
    "class_weight = median / frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    \n",
    "    intersection = torch.zeros(8)   # the dataset has 8 classes including background\n",
    "    union = torch.zeros(8)\n",
    "    \n",
    "    for sample in test_loader:\n",
    "        x, y = sample['image'], sample['class']\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, ypred = model(x).max(1)    # y_pred.shape => (N, 240, 320)\n",
    "        \n",
    "        for i in range(8):\n",
    "            y_i = (y == i)           \n",
    "            ypred_i = (ypred == i)   \n",
    "            \n",
    "            inter = (y_i.byte() & ypred_i.byte()).float().sum().to('cpu')\n",
    "            intersection[i] += inter\n",
    "            union[i] += (y_i.float().sum() + ypred_i.float().sum()).to('cpu') - inter\n",
    "    \n",
    "    \"\"\" iou[i] is the IoU of class i \"\"\"\n",
    "    iou = intersection / union\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, optimizer_cls=optim.Adam, \n",
    "                criterion=nn.CrossEntropyLoss(), max_epoch=200, device='cpu', writer=None):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_iou = []\n",
    "    mean_iou = []\n",
    "    best_iou = 0.0\n",
    "    \n",
    "    optimizer = optimizer_cls(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, sample in tqdm.tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x, y = sample['image'], sample['class']\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            h = model(x)\n",
    "            loss = criterion(h, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_loss / i)\n",
    "        \n",
    "        val_iou.append(eval_model(model, test_loader, device))\n",
    "        mean_iou.append(val_iou[-1].mean().item())\n",
    "        \n",
    "        if best_iou < mean_iou[-1]:\n",
    "            best_iou = mean_iou[-1]\n",
    "            torch.save(model.state_dict(), \"./SegNet_with_class_weight(median)_results/best_iou_model.prm\")\n",
    "        \n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"train_loss\", train_losses[-1], epoch)\n",
    "            writer.add_scalar(\"mean_IoU\", mean_iou[-1], epoch)\n",
    "            writer.add_scalars(\"class_IoU\", {'iou of class 0': val_iou[-1][0],\n",
    "                                           'iou of class 1': val_iou[-1][1],\n",
    "                                           'iou of class 2': val_iou[-1][2],\n",
    "                                           'iou of class 3': val_iou[-1][3],\n",
    "                                           'iou of class 4': val_iou[-1][4],\n",
    "                                           'iou of class 5': val_iou[-1][5],\n",
    "                                           'iou of class 6': val_iou[-1][6],\n",
    "                                           'iou of class 7': val_iou[-1][7]}, epoch)\n",
    "            \n",
    "        print(epoch, train_losses[-1], mean_iou[-1])\n",
    "        \n",
    "    torch.save(model.state_dict(), \"./SegNet_with_class_weight(median)_results/final_model.prm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SegNetBasic(3, 8)\n",
    "# writer = SummaryWriter(\"./SegNet_with_class_weight(median)_results/\")\n",
    "# train_model(model, train_loader, test_loader, criterion=nn.CrossEntropyLoss(weight=class_weight.to('cuda')), device=\"cuda\", writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = torch.tensor([[0, 0, 0],         # class 0 'background'  black\n",
    "                       [255, 0, 0],       # class 1 'grasp'       red\n",
    "                       [255, 255, 0],     # class 2 'cut'         yellow\n",
    "                       [0, 255, 0],       # class 3 'scoop'       green\n",
    "                       [0, 255, 255],     # class 4 'contain'     sky blue\n",
    "                       [0, 0, 255],       # class 5 'pound'       blue\n",
    "                       [255, 0, 255],     # class 6 'support'     purple\n",
    "                       [255, 255, 255]    # class 7 'wrap grasp'  white\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_to_mask(cls):\n",
    "    \n",
    "    mask = colors[cls].transpose(1, 2).transpose(1, 3)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sample, device='cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    x, y = sample['image'], sample['class']\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, y_pred = model(x).max(1)    # y_pred.shape => (N, 240, 320)\n",
    "    \n",
    "    true_mask = class_to_mask(y).to('cpu')\n",
    "    pred_mask = class_to_mask(y_pred).to('cpu')\n",
    "    \n",
    "    save_image(x.to('cpu'), \"./original_img_with_SegNet.jpg\")\n",
    "    save_image(true_mask, \"./true_mask_with_SegNet.jpg\")\n",
    "    save_image(pred_mask, \"./pred_mask_with_SegNet.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = SegNetBasic(3, 8)\n",
    "trained_model.load_state_dict(torch.load(\"./SegNet_with_class_weights_results/final_model.prm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdfasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(10, 256, 320).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label, n_classes):\n",
    "    one_hot_label = torch.eye(n_classes)[label].transpose(1, 3).transpose(2, 3)\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 8, 256, 320])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = one_hot(x, 8)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\tloss_full: 2.0\tloss_D: 3.0\tloss_semi: 4.0\tmean IOU: 5.0\n"
     ]
    }
   ],
   "source": [
    "print('epoch: {}\\tloss_full: {}\\tloss_D: {}\\tloss_semi: {}\\tmean IOU: {}'\n",
    "    .format(1, 2., 3., 4., 5.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
