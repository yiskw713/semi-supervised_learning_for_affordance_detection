{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import skimage.io\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import click\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from models.deeplabv2 import DeepLabV2\n",
    "from models.msc import MSC\n",
    "from models.discriminator import Discriminator\n",
    "from dataset import PartAffordanceDataset, PartAffordanceDatasetWithoutLabel\n",
    "from dataset import CenterCrop, ToTensor, Normalize\n",
    "\n",
    "\n",
    "\n",
    "''' one-hot representation '''\n",
    "\n",
    "def one_hot(label, n_classes):\n",
    "    one_hot_label = torch.eye(n_classes, requires_grad=True)[label].transpose(1, 3).transpose(2, 3)\n",
    "    return one_hot_label\n",
    "    \n",
    "\n",
    "''' scheduler for learning rate '''\n",
    "\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter, max_iter, power):\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return None\n",
    "    new_lr = init_lr * (1 - float(iter) / max_iter) ** power\n",
    "    optimizer.param_groups[0][\"lr\"] = new_lr\n",
    "    optimizer.param_groups[1][\"lr\"] = 10 * new_lr\n",
    "    optimizer.param_groups[2][\"lr\"] = 20 * new_lr\n",
    "\n",
    "\n",
    "def poly_lr_scheduler_d(optimizer, init_lr, iter, lr_decay_iter, max_iter, power):\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return None\n",
    "    new_lr = init_lr * (1 - float(iter) / max_iter) ** power\n",
    "    optimizer.param_groups[0][\"lr\"] = new_lr\n",
    "    if len(optimizer.param_groups) > 1:\n",
    "        optimizer.param_groups[1]['lr'] = 10 * new_lr\n",
    "\n",
    "\n",
    "''' model, weight initialization, get params '''\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "\n",
    "\n",
    "def DeepLabV2_ResNet101_MSC(n_classes):\n",
    "    return MSC(\n",
    "        scale=DeepLabV2(\n",
    "            n_classes=n_classes, n_blocks=[3, 4, 23, 3], pyramids=[6, 12, 18, 24]\n",
    "        ),\n",
    "        pyramids=[0.5, 0.75],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_params(model, key):\n",
    "    # For Dilated FCN\n",
    "    if key == \"1x\":\n",
    "        for m in model.named_modules():\n",
    "            if \"layer\" in m[0]:\n",
    "                if isinstance(m[1], nn.Conv2d):\n",
    "                    for p in m[1].parameters():\n",
    "                        yield p\n",
    "    # For conv weight in the ASPP module\n",
    "    if key == \"10x\":\n",
    "        for m in model.named_modules():\n",
    "            if \"aspp\" in m[0]:\n",
    "                if isinstance(m[1], nn.Conv2d):\n",
    "                    yield m[1].weight\n",
    "    # For conv bias in the ASPP module\n",
    "    if key == \"20x\":\n",
    "        for m in model.named_modules():\n",
    "            if \"aspp\" in m[0]:\n",
    "                if isinstance(m[1], nn.Conv2d):\n",
    "                    yield m[1].bias\n",
    "\n",
    "\n",
    "\n",
    "''' training '''\n",
    "\n",
    "def full_train(\n",
    "        model, model_d, sample, criterion_ce_full, criterion_bce, \n",
    "        optimizer, optimizer_d, ones, zeros, device):\n",
    "\n",
    "    ''' full supervised learning '''\n",
    "    \n",
    "    model.train()\n",
    "    model.scale.freeze_bn()\n",
    "    model_d.train()\n",
    "\n",
    "    # train segmentation network\n",
    "    x, y = sample['image'], sample['class']\n",
    "\n",
    "    batch_len = len(x)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    h = model(x)     # shape => (N, 8, H/8, W/8)\n",
    "    h = F.interpolate(h, size=(256, 320), mode='bilinear', align_corners=True)\n",
    "\n",
    "    h_ = h.detach().to(device)    # h_ is for calculating loss for discriminator\n",
    "    y_ = y.detach().to(device)    # y_is for the same purpose\n",
    "    \n",
    "    print(h_.dtype)\n",
    "    \n",
    "    d_out = model_d(h)    # shape => (N, 1, H/32, W/32)\n",
    "    d_out = F.interpolate(d_out, size=(256, 320), mode='bilinear', align_corners=True)    # shape => (N, 1, H, W)\n",
    "    d_out = d_out.squeeze()\n",
    "    \n",
    "    loss_ce = criterion_ce_full(h, y)\n",
    "    loss_adv = criterion_bce(d_out, ones[:batch_len])\n",
    "    loss_full = loss_ce + 0.01 * loss_adv\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_d.zero_grad()\n",
    "    loss_full.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # train discriminator\n",
    "    seg_out = model_d(h_)    # shape => (N, 1, H/32, W/32)\n",
    "    seg_out = F.interpolate(seg_out, size=(256, 320), mode='bilinear', align_corners=True)    # shape => (N, 1, H, W)\n",
    "    seg_out = seg_out.squeeze()\n",
    "    \n",
    "    print(y_.dtype)\n",
    "    y_ = one_hot(y_, 8)    # shape => (N, 8, H, W)\n",
    "    print(y_.dtype)\n",
    "    true_out = model_d(y_)    # shape => (N, 1, H/32, W/32)\n",
    "    true_out = F.interpolate(true_out, size=(256, 320), mode='bilinear', align_corners=True)    # shape => (N, 1, H, W)\n",
    "    true_out = true_out.squeeze()\n",
    "\n",
    "    loss_d_fake = criterion_bce(seg_out, zeros[:batch_len])\n",
    "    loss_d_real = criterion_bce(true_out, ones[:batch_len])\n",
    "    loss_d = loss_d_fake + loss_d_real\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_d.zero_grad()\n",
    "    loss_d.backward()\n",
    "    optimizer_d.step()\n",
    "\n",
    "    return loss_full.item(), loss_d.item()\n",
    "\n",
    "\n",
    "\n",
    "def semi_train(\n",
    "        model, model_d, sample, criterion_ce_semi, criterion_bce, \n",
    "        optimizer, optimizer_d, ones, zeros, device):\n",
    "\n",
    "    ''' semi supervised learning '''\n",
    "    \n",
    "    model.train()\n",
    "    model.scale.freeze_bn()\n",
    "    model_d.eval()\n",
    "\n",
    "    # train segmentation network\n",
    "    x = sample['image']\n",
    "    batch_len = len(x)\n",
    "    \n",
    "    x = x.to(device)\n",
    "\n",
    "    h = model(x)     # shape => (N, 8, H/8, W/8)\n",
    "    h = F.interpolate(h, size=(256, 320), mode='bilinear', align_corners=True)\n",
    "\n",
    "    h_, _ = torch.max(h, dim=1)    # to calculate the crossentropy loss. shape => (N, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        d_out = model_d(h)    # shape => (N, 1, H/32, W/32)\n",
    "        d_out = F.interpolate(d_out, size=(256, 320), mode='bilinear', align_corners=True)    # shape => (N, 1, H, W)\n",
    "        d_out = d_out.squeeze()\n",
    "\n",
    "    loss_adv = criterion_bce(d_out, ones[:batch_len])\n",
    "\n",
    "\n",
    "    # if the pixel value of the output from discriminator is more than a threshold,\n",
    "    # its value is viewd as one from true label. Else, its value is ignored(value=255).\n",
    "    h_[d_out < 0.2] = 255\n",
    "\n",
    "    loss_ce = criterion_ce_semi(h, h_)\n",
    "    loss_semi = 0.001 * loss_adv + 0.1 * loss_ce\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_d.zero_grad()\n",
    "    loss_semi.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss_semi.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = './models/deeplabv2_resnet101_COCO_init.pth'\n",
    "class_weight_flag = True\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "max_epoch = 100\n",
    "learning_rate = 0.00025\n",
    "learning_rate_d = 0.0001\n",
    "n_classes = 8\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "''' DataLoader '''\n",
    "train_data_with_label = PartAffordanceDataset('train_with_label.csv',\n",
    "                                        transform=transforms.Compose([\n",
    "                                            CenterCrop(),\n",
    "                                            ToTensor(),\n",
    "                                            Normalize()\n",
    "                                        ]))\n",
    "\n",
    "train_data_without_label = PartAffordanceDatasetWithoutLabel('train_with_label.csv',\n",
    "                                        transform=transforms.Compose([\n",
    "                                            CenterCrop(),\n",
    "                                            ToTensor(),\n",
    "                                            Normalize()\n",
    "                                         ]))\n",
    "\n",
    "test_data = PartAffordanceDataset('test.csv',\n",
    "                            transform=transforms.Compose([\n",
    "                                CenterCrop(),\n",
    "                                ToTensor(),\n",
    "                                Normalize()\n",
    "                            ]))\n",
    "\n",
    "train_loader_with_label = DataLoader(train_data_with_label, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "train_loader_without_label = DataLoader(train_data_without_label, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "''' define model, optimizer, loss '''\n",
    "model = DeepLabV2_ResNet101_MSC(n_classes)\n",
    "model_d = Discriminator(n_classes)\n",
    "\n",
    "model.apply(init_weights)\n",
    "model_d.apply(init_weights)\n",
    "\n",
    "state_dict = torch.load(pretrained_model)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "model.to(device)\n",
    "model_d.to(device)\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "                    params=[{\n",
    "                        \"params\": get_params(model, key=\"1x\"),\n",
    "                        \"lr\": learning_rate,\n",
    "                        \"weight_decay\": 5.0e-4,\n",
    "                            },\n",
    "                            {\n",
    "                        \"params\": get_params(model, key=\"10x\"),\n",
    "                        \"lr\": 10 * learning_rate,\n",
    "                        \"weight_decay\": 5.0e-4,\n",
    "                            },\n",
    "                            {\n",
    "                            \"params\": get_params(model, key=\"20x\"),\n",
    "                            \"lr\": 20 * learning_rate,\n",
    "                            \"weight_decay\": 0.0,\n",
    "                            }],\n",
    "                    momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "optimizer_d = optim.Adam(model_d.parameters(), lr=learning_rate_d, betas=(0.9,0.99))\n",
    "\n",
    "if class_weight_flag:\n",
    "    class_weight = torch.tensor([0.0057, 0.4689, 1.0000, 1.2993, \n",
    "                                0.4240, 2.3702, 1.7317, 0.8149])    # refer to dataset.py\n",
    "    criterion_ce_full = nn.CrossEntropyLoss(weight=class_weight.to(device))\n",
    "else:\n",
    "    criterion_ce_full = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion_ce_semi = nn.CrossEntropyLoss(ignore_index=255)\n",
    "criterion_bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# supplementary constant for discriminator\n",
    "ones = torch.ones(batch_size, 256, 320).to(device)\n",
    "zeros = torch.zeros(batch_size, 256, 320).to(device)\n",
    "\n",
    "\n",
    "''' training '''\n",
    "\n",
    "losses_full = []\n",
    "losses_semi = []\n",
    "losses_d = []\n",
    "val_iou = []\n",
    "mean_iou = []\n",
    "best_iou = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/yuchi/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int64\n",
      "torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-efd9ff9f3857>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m             loss_full, loss_d = full_train(\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_ce_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_bce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                                     optimizer, optimizer_d, ones, zeros, device)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mepoch_loss_full\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-84d5cf4de7d3>\u001b[0m in \u001b[0;36mfull_train\u001b[0;34m(model, model_d, sample, criterion_ce_full, criterion_bce, optimizer, optimizer_d, ones, zeros, device)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0my_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# shape => (N, 8, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mtrue_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# shape => (N, 1, H/32, W/32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mtrue_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m320\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# shape => (N, 1, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mtrue_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/semi-supervised_learning_for_affordance_detection/models/discriminator.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight'"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.tqdm(range(max_epoch)):\n",
    "\n",
    "    epoch_loss_full = 0.0\n",
    "    epoch_loss_d = 0.0\n",
    "    epoch_loss_semi = 0.0\n",
    "\n",
    "    poly_lr_scheduler(\n",
    "        optimizer=optimizer,\n",
    "        init_lr=learning_rate,\n",
    "        iter=epoch - 1,\n",
    "        lr_decay_iter=10,\n",
    "        max_iter=max_epoch,\n",
    "        power=0.9,\n",
    "    )\n",
    "\n",
    "    poly_lr_scheduler_d(\n",
    "        optimizer=optimizer_d,\n",
    "        init_lr=learning_rate_d,\n",
    "        iter=epoch - 1,\n",
    "        lr_decay_iter=10,\n",
    "        max_iter=max_epoch,\n",
    "        power=0.9,\n",
    "    )\n",
    "\n",
    "\n",
    "    # only supervised learning\n",
    "    if epoch < 10:\n",
    "        for i, sample in enumerate(train_loader_with_label):\n",
    "\n",
    "            loss_full, loss_d = full_train(\n",
    "                                    model, model_d, sample, criterion_ce_full, criterion_bce,\n",
    "                                    optimizer, optimizer_d, ones, zeros, device)\n",
    "\n",
    "            epoch_loss_full += loss_full\n",
    "            epoch_loss_d += loss_d\n",
    "\n",
    "        losses_full.append(epoch_loss_full / i)   # mean loss over all samples\n",
    "        losses_d.append(epoch_loss_d / i)\n",
    "        losses_semi.append(0.0)\n",
    "\n",
    "\n",
    "    # semi-supervised learning\n",
    "    if epoch >= 10:\n",
    "        for i, (sample1, sample2) in enumerate(zip(train_loader_with_label, train_loader_without_label)):\n",
    "\n",
    "            loss_full, loss_d = full_train(\n",
    "                                    model, model_d, sample1, criterion_ce_full, criterion_bce,\n",
    "                                    optimizer, optimizer_d, ones, zeros, device)\n",
    "\n",
    "            epoch_loss_full += loss_full\n",
    "            epoch_loss_d += loss_d\n",
    "\n",
    "            loss_semi += semi_train(\n",
    "                                    model, model_d, sample2, criterion_ce_semi, criterion_bce,\n",
    "                                    optimizer, optimizer_d, ones, zeros, device)\n",
    "\n",
    "            epoch_loss_semi += loss_semi\n",
    "\n",
    "        losses_full.append(epoch_loss_full / i)   # mean loss over all samples\n",
    "        losses_d.append(epoch_loss_d / i)\n",
    "        losses_semi.append(epoch_loss_semi / i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
