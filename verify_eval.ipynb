{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import scipy.io\n",
    "import skimage.io\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from addict import Dict\n",
    "from PIL import Image, ImageFilter\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from models.SegNet import SegNetBasic\n",
    "from models.discriminator import Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartAffordanceDataset(Dataset):\n",
    "    \"\"\"Part Affordance Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_class_path = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_class_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_class_path.iloc[idx, 0]\n",
    "        class_path = self.image_class_path.iloc[idx, 1]\n",
    "        image = skimage.io.imread(image_path)\n",
    "        cls = scipy.io.loadmat(class_path)[\"gt_label\"]\n",
    "        \n",
    "        sample = {'image': image, 'class': cls}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class PartAffordanceDatasetWithoutLabel(Dataset):\n",
    "    \"\"\" Part Affordance Dataset without label \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_path = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_path.iloc[idx, 0]\n",
    "        image = Image.open(image_path) \n",
    "        \n",
    "        sample = {'image': image}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_numpy(array, crop_height, crop_weight):\n",
    "    h, w = array.shape\n",
    "    return array[h//2 - crop_height//2: h//2 + crop_height//2,\n",
    "                w//2 - crop_weight//2: w//2 + crop_weight//2\n",
    "                ]\n",
    "\n",
    "\n",
    "def crop_center_pil_image(pil_img, crop_height, crop_width):\n",
    "    w, h = pil_img.size\n",
    "    return pil_img.crop(((w - crop_width) // 2,\n",
    "                        (h - crop_height) // 2,\n",
    "                        (w + crop_width) // 2,\n",
    "                        (h + crop_height) // 2))\n",
    "\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if 'class' in sample:\n",
    "            image, cls = sample['image'], sample['class']\n",
    "            image = crop_center_pil_image(image, 256, 320)\n",
    "            cls = crop_center_numpy(cls, 256, 320)\n",
    "            return {'image': image, 'class': cls}\n",
    "            \n",
    "        else:\n",
    "            image = sample['image']\n",
    "            image = crop_center_pil_image(image, 256, 320)\n",
    "            return {'image': image}\n",
    "\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        if 'class' in sample:\n",
    "            image, cls = sample['image'], sample['class']\n",
    "            return {'image': torch.from_numpy(image).float(), \n",
    "                    'class': torch.from_numpy(cls).long()}\n",
    "        else:\n",
    "            image = sample['image']\n",
    "            return {'image': transforms.functional.to_tensor(image).float()}\n",
    "\n",
    "\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self, mean=[55.8630, 59.9099, 91.7419], std=[31.6852, 29.8496, 19.0835]):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        if 'class' in sample:\n",
    "            image, cls = sample['image'], sample['class']\n",
    "            image = transforms.functional.normalize(image, self.mean, self.std)\n",
    "            return {'image': image, 'class': cls}\n",
    "        else:\n",
    "            image = sample['image']\n",
    "            image = transforms.functional.normalize(image, self.mean, self.std)\n",
    "            return {'image': image}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegNetBasic(3, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train(model, sample, criterion_ce_full, optimizer, device):\n",
    "\n",
    "    ''' full supervised learning for segmentation network'''\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    x, y = sample['image'], sample['class']\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    h = model(x)     # shape => (N, 8, H, W)\n",
    "\n",
    "    loss_ce = criterion_ce_full(h, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_ce.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss_ce.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, device='cpu'):\n",
    "    model.eval()\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    intersection = torch.zeros(8)   # the dataset has 8 classes including background\n",
    "    union = torch.zeros(8)\n",
    "    \n",
    "    for k, sample in enumerate(test_loader):\n",
    "        x, y = sample['image'], sample['class']\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ypred = model(x)    # ypred.shape => (N, 8, H, W)\n",
    "            _, ypred = ypred.max(1)    # y_pred.shape => (N, 256, 320)\n",
    "\n",
    "        for i in range(8):\n",
    "            y_i = (y == i)           \n",
    "            ypred_i = (ypred == i)   \n",
    "            \n",
    "            inter = (y_i.byte() & ypred_i.byte()).float().sum().to('cpu')\n",
    "            intersection[i] += inter\n",
    "            union[i] += (y_i.float().sum() + ypred_i.float().sum()).to('cpu') - inter\n",
    "            \n",
    "        if k == 10:\n",
    "            break\n",
    "    \n",
    "        \n",
    "    \"\"\" iou[i] is the IoU of class i \"\"\"\n",
    "    iou = intersection / union\n",
    "    \n",
    "    taken_time = start - time.time()\n",
    "    \n",
    "    print(taken_time)\n",
    "    \n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = Dict(yaml.safe_load(open('./result_segnet/config_segnet.yaml')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_with_label = PartAffordanceDataset('train.csv',\n",
    "                                        transform=transforms.Compose([\n",
    "                                            CenterCrop(),\n",
    "                                            ToTensor(),\n",
    "                                            Normalize()\n",
    "                                        ]))\n",
    "\n",
    "train_data_without_label = PartAffordanceDatasetWithoutLabel('train_without_label_4to1.csv',\n",
    "                                        transform=transforms.Compose([\n",
    "                                            CenterCrop(),\n",
    "                                            ToTensor(),\n",
    "                                            Normalize()\n",
    "                                        ]))\n",
    "\n",
    "test_data = PartAffordanceDataset('test.csv',\n",
    "                            transform=transforms.Compose([\n",
    "                                ToTensor(),\n",
    "                            ]))\n",
    "\n",
    "train_loader_with_label = DataLoader(train_data_with_label, batch_size=CONFIG.batch_size, shuffle=True, num_workers=CONFIG.num_workers)\n",
    "train_loader_without_label = DataLoader(train_data_without_label, batch_size=CONFIG.batch_size, shuffle=True, num_workers=CONFIG.num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size=CONFIG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label, n_classes, device):\n",
    "    one_hot_label = torch.eye(n_classes, requires_grad=True, device=device)[label].transpose(1, 3).transpose(2, 3)\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, test_loader, device='cpu'):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    intersections = torch.zeros(8).to(device)\n",
    "    unions = torch.zeros(8).to(device)\n",
    "    \n",
    "    for i, sample in enumerate(test_loader):\n",
    "        x = sample['image']\n",
    "        y = sample['class']\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            ypred = model(x)    # ypred.shape => (N, 8, H, W)\n",
    "            _, ypred = ypred.max(1)    # y_pred.shape => (N, 256, 320)\n",
    "\n",
    "        p = one_hot(ypred, 8, device).long()\n",
    "        t = one_hot(y, 8, device).long()\n",
    "        \n",
    "        intersection = torch.sum(p & t, (0,2,3))\n",
    "        union = torch.sum(p | t, (0, 2, 3))\n",
    "        \n",
    "        intersections += intersection.float()\n",
    "        unions += union.float()\n",
    "        \n",
    "        if i == 10:\n",
    "            break\n",
    "        \n",
    "    iou = intersections / unions\n",
    "    \n",
    "    taken_time = time.time() - start\n",
    "    print(taken_time)\n",
    "    \n",
    "    return iou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
