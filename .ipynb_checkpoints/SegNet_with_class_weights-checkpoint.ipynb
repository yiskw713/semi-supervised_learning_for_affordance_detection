{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import skimage.io\n",
    "\n",
    "from PIL import Image, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x, idx = F.max_pool2d(x, kernel_size=2, stride=2, return_indices=True)\n",
    "        return x, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNetBasic(nn.Module):\n",
    "    \"\"\" \n",
    "        SegNet Basic is a smaller version of SegNet\n",
    "        Please refer to this repository:\n",
    "        https://github.com/0bserver07/Keras-SegNet-Basic\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder1 = Encoder(in_channel, 64)\n",
    "        self.encoder2 = Encoder(64, 80)\n",
    "        self.encoder3 = Encoder(80, 96)\n",
    "        self.encoder4 = Encoder(96, 128)\n",
    "        \n",
    "        self.decoder1 = Decoder(128, 96)\n",
    "        self.decoder2 = Decoder(96, 80)\n",
    "        self.decoder3 = Decoder(80, 64)\n",
    "        self.decoder4 = Decoder(64, out_channel)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size1 = x.size()\n",
    "        x, idx1 = self.encoder1(x)\n",
    "\n",
    "        size2 = x.size()\n",
    "        x, idx2 = self.encoder2(x)\n",
    "\n",
    "        size3 = x.size()\n",
    "        x, idx3 = self.encoder3(x)\n",
    "        \n",
    "        size4 = x.size()\n",
    "        x, idx4 = self.encoder4(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx4, kernel_size=2, stride=2, output_size=size4)\n",
    "        x = self.decoder1(x)\n",
    "        \n",
    "        x = F.max_unpool2d(x, idx3, kernel_size=2, stride=2, output_size=size3)\n",
    "        x = self.decoder2(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx2, kernel_size=2, stride=2, output_size=size2)\n",
    "        x = self.decoder3(x)\n",
    "\n",
    "        x = F.max_unpool2d(x, idx1, kernel_size=2, stride=2, output_size=size1)\n",
    "        x = self.decoder4(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartAffordanceDataset(Dataset):\n",
    "    \"\"\"Part Affordance Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_class_path = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_class_path)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_class_path.iloc[idx, 0]\n",
    "        class_path = self.image_class_path.iloc[idx, 1]\n",
    "        image = skimage.io.imread(image_path) # read as numpy array\n",
    "        cls = scipy.io.loadmat(class_path)[\"gt_label\"]\n",
    "        \n",
    "        sample = {'image': image, 'class': cls}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_numpy(array, crop_height, crop_weight):\n",
    "    h, w = array.shape\n",
    "    return array[h//2 - crop_height//2: h//2 + crop_height//2,\n",
    "                 w//2 - crop_weight//2: w//2 + crop_weight//2\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center_pil_image(pil_img, crop_width, crop_height):\n",
    "    img_width, img_height = pil_img.size\n",
    "    return pil_img.crop(((img_width - crop_width) // 2,\n",
    "                         (img_height - crop_height) // 2,\n",
    "                         (img_width + crop_width) // 2,\n",
    "                         (img_height + crop_height) // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterCrop(object):\n",
    "    def __call__(self, sample):\n",
    "        image, cls = sample['image'], sample['class']\n",
    "        \n",
    "        image = Image.fromarray(np.uint8(image))\n",
    "        \n",
    "        image = crop_center_pil_image(image, 320, 240)\n",
    "        cls = crop_center_numpy(cls, 240, 320)\n",
    "        \n",
    "        image = np.asarray(image)\n",
    "        \n",
    "        return {'image': image, 'class': cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, cls = sample['image'], sample['class']\n",
    "        \n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image).float(), \n",
    "                'class': torch.from_numpy(cls).long()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[55.8630, 59.9099, 91.7419]\n",
    "std=[31.6852, 29.8496, 19.0835]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    def __call__(self, sample):\n",
    "        image, cls = sample['image'], sample['class']\n",
    "        \n",
    "        image = transforms.functional.normalize(image, mean, std)\n",
    "        \n",
    "        return {'image': image, 'class': cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PartAffordanceDataset('train.csv',\n",
    "                                transform=transforms.Compose([\n",
    "                                    CenterCrop(),\n",
    "                                    ToTensor(),\n",
    "                                    Normalize()\n",
    "                                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = PartAffordanceDataset('test.csv',\n",
    "                                transform=transforms.Compose([\n",
    "                                    CenterCrop(),\n",
    "                                    ToTensor(),\n",
    "                                    Normalize()\n",
    "                                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = torch.tensor([2078085712, 34078992, 15921090, 12433420, \n",
    "                          38473752, 6773528, 9273826, 20102080])\n",
    "\n",
    "class_weight = class_num[0].float() / (100.0 * class_num.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_model(model, test_loader, device='cpu'):\n",
    "#     model.eval()\n",
    "    \n",
    "#     intersection = torch.zeros(8)   # the dataset has 8 classes including background\n",
    "#     union = torch.zeros(8)\n",
    "    \n",
    "#     for sample in test_loader:\n",
    "#         x, y = sample['image'], sample['class']\n",
    "        \n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             _, ypred = model(x).max(1)    # y_pred.shape => (N, 240, 320)\n",
    "        \n",
    "#         for i in range(8):\n",
    "#             y_i = (y == i)           \n",
    "#             ypred_i = (ypred == i)   \n",
    "            \n",
    "#             inter = (y_i.byte() & ypred_i.byte()).float().sum().to('cpu')\n",
    "#             intersection[i] += inter\n",
    "#             union[i] += (y_i.float().sum() + ypred_i.float().sum()).to('cpu') - inter\n",
    "    \n",
    "#     \"\"\" iou[i] is the IoU of class i \"\"\"\n",
    "#     iou = intersection / union\n",
    "    \n",
    "#     return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, train_loader, test_loader, optimizer_cls=optim.Adam, \n",
    "#                 criterion=nn.CrossEntropyLoss(), max_epoch=200, device='cpu', writer=None):\n",
    "    \n",
    "#     model.to(device)\n",
    "    \n",
    "#     train_losses = []\n",
    "#     val_iou = []\n",
    "#     mean_iou = []\n",
    "#     best_iou = 0.0\n",
    "    \n",
    "#     optimizer = optimizer_cls(model.parameters(), lr=0.01)\n",
    "    \n",
    "#     for epoch in range(max_epoch):\n",
    "#         model.train()\n",
    "#         running_loss = 0.0\n",
    "        \n",
    "#         for i, sample in tqdm.tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             x, y = sample['image'], sample['class']\n",
    "            \n",
    "#             x = x.to(device)\n",
    "#             y = y.to(device)\n",
    "\n",
    "#             h = model(x)\n",
    "#             loss = criterion(h, y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         train_losses.append(running_loss / i)\n",
    "        \n",
    "#         val_iou.append(eval_model(model, test_loader, device))\n",
    "#         mean_iou.append(val_iou[-1].mean().item())\n",
    "        \n",
    "#         if best_iou < mean_iou[-1]:\n",
    "#             best_iou = mean_iou[-1]\n",
    "#             torch.save(model.state_dict(), \"./SegNet_with_class_weight(median)_results/best_iou_model.prm\")\n",
    "        \n",
    "#         if writer is not None:\n",
    "#             writer.add_scalar(\"train_loss\", train_losses[-1], epoch)\n",
    "#             writer.add_scalar(\"mean_IoU\", mean_iou[-1], epoch)\n",
    "#             writer.add_scalars(\"class_IoU\", {'iou of class 0': val_iou[-1][0],\n",
    "#                                            'iou of class 1': val_iou[-1][1],\n",
    "#                                            'iou of class 2': val_iou[-1][2],\n",
    "#                                            'iou of class 3': val_iou[-1][3],\n",
    "#                                            'iou of class 4': val_iou[-1][4],\n",
    "#                                            'iou of class 5': val_iou[-1][5],\n",
    "#                                            'iou of class 6': val_iou[-1][6],\n",
    "#                                            'iou of class 7': val_iou[-1][7]}, epoch)\n",
    "            \n",
    "#         print(epoch, train_losses[-1], mean_iou[-1])\n",
    "        \n",
    "#     torch.save(model.state_dict(), \"./SegNet_with_class_weight(median)_results/final_model.prm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = torch.tensor([[0, 0, 0],         # class 0 'background'  black\n",
    "                       [255, 0, 0],       # class 1 'grasp'       red\n",
    "                       [255, 255, 0],     # class 2 'cut'         yellow\n",
    "                       [0, 255, 0],       # class 3 'scoop'       green\n",
    "                       [0, 255, 255],     # class 4 'contain'     sky blue\n",
    "                       [0, 0, 255],       # class 5 'pound'       blue\n",
    "                       [255, 0, 255],     # class 6 'support'     purple\n",
    "                       [255, 255, 255]    # class 7 'wrap grasp'  white\n",
    "                      ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_to_mask(cls):\n",
    "    \n",
    "    mask = colors[cls].transpose(1, 2).transpose(1, 3)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, sample, device='cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    x, y = sample['image'], sample['class']\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, y_pred = model(x).max(1)    # y_pred.shape => (N, 240, 320)\n",
    "    \n",
    "    true_mask = class_to_mask(y).to('cpu')\n",
    "    pred_mask = class_to_mask(y_pred).to('cpu')\n",
    "    \n",
    "    save_image(true_mask, \"./SegNet_with_class_weight_results/true_mask_with_SegNet_with_class_weight.jpg\")\n",
    "    save_image(pred_mask, \"./SegNet_with_class_weight_results/pred_mask_with_SegNet_with_class_weight.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = SegNetBasic(3, 8)\n",
    "trained_model.load_state_dict(torch.load(\"./SegNet_with_class_weight_results/best_iou_model.prm\",\n",
    "                                        map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = PartAffordanceDataset('eval.csv',\n",
    "                                transform=transforms.Compose([\n",
    "                                    CenterCrop(),\n",
    "                                    ToTensor(),\n",
    "                                    Normalize()\n",
    "                                ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_normalize(x, mean, std):\n",
    "    x[:, 0, :, :] = x[:, 0, :, :] * std[0] + mean[0]\n",
    "    x[:, 1, :, :] = x[:, 1, :, :] * std[1] + mean[1]\n",
    "    x[:, 2, :, :] = x[:, 2, :, :] * std[2] + mean[2]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loader = DataLoader(eval_data, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=[55.8630, 59.9099, 91.7419]\n",
    "std=[31.6852, 29.8496, 19.0835]\n",
    "\n",
    "for sample in eval_loader:\n",
    "    trained_model.eval()\n",
    "    \n",
    "    predict(trained_model, sample)\n",
    "    \n",
    "    x = sample[\"image\"]\n",
    "    x = reverse_normalize(x, mean, std)\n",
    "    save_image(x/255, \"./SegNet_with_class_weight_results/original_img_with_SegNet_with_class_weight.jpg\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
