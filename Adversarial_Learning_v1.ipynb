{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import sys\n",
    "import tqdm\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "from models.deeplabv2 import DeepLabV2\n",
    "from models.msc import MSC\n",
    "from models.discriminator import Discriminator\n",
    "from dataset import PartAffordanceDataset, PartAffordanceDatasetWithoutLabel\n",
    "from dataset import CenterCrop, ToTensor, Normalize\n",
    "\n",
    "\n",
    "\n",
    "''' one-hot representation '''\n",
    "\n",
    "def one_hot(label, n_classes, device):\n",
    "    one_hot_label = torch.eye(n_classes, requires_grad=True, device=device)[label].transpose(1, 3).transpose(2, 3)\n",
    "    return one_hot_label\n",
    "    \n",
    "\n",
    "''' scheduler for learning rate '''\n",
    "\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter, max_iter, power):\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return None\n",
    "    new_lr = init_lr * (1 - float(iter) / max_iter) ** power\n",
    "    optimizer.param_groups[0][\"lr\"] = new_lr\n",
    "    optimizer.param_groups[1][\"lr\"] = 10 * new_lr\n",
    "    optimizer.param_groups[2][\"lr\"] = 20 * new_lr\n",
    "\n",
    "\n",
    "def poly_lr_scheduler_d(optimizer, init_lr, iter, lr_decay_iter, max_iter, power):\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return None\n",
    "    new_lr = init_lr * (1 - float(iter) / max_iter) ** power\n",
    "    optimizer.param_groups[0][\"lr\"] = new_lr\n",
    "    if len(optimizer.param_groups) > 1:\n",
    "        optimizer.param_groups[1]['lr'] = 10 * new_lr\n",
    "\n",
    "\n",
    "''' model, weight initialization, get params '''\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "\n",
    "\n",
    "def DeepLabV2_ResNet101_MSC(n_classes):\n",
    "    return MSC(\n",
    "        scale=DeepLabV2(\n",
    "            n_classes=n_classes, n_blocks=[3, 4, 23, 3], pyramids=[6, 12, 18, 24]\n",
    "        ),\n",
    "        pyramids=[0.5, 0.75],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_params(model, key):\n",
    "    # For Dilated FCN\n",
    "    if key == \"1x\":\n",
    "        for m in model.named_modules():\n",
    "            if \"layer\" in m[0]:\n",
    "                if isinstance(m[1], nn.Conv2d):\n",
    "                    for p in m[1].parameters():\n",
    "                        yield p\n",
    "    # For conv weight in the ASPP module\n",
    "    if key == \"10x\":\n",
    "        for m in model.named_modules():\n",
    "            if \"aspp\" in m[0]:\n",
    "                if isinstance(m[1], nn.Conv2d):\n",
    "                    yield m[1].weight\n",
    "    # For conv bias in the ASPP module\n",
    "    if key == \"20x\":\n",
    "        for m in model.named_modules():\n",
    "            if \"aspp\" in m[0]:\n",
    "                if isinstance(m[1], nn.Conv2d):\n",
    "                    yield m[1].bias\n",
    "\n",
    "\n",
    "\n",
    "''' training '''\n",
    "\n",
    "def full_train(\n",
    "        model, model_d, sample, criterion_ce_full, criterion_bce, \n",
    "        optimizer, optimizer_d, ones, zeros, device):\n",
    "\n",
    "    ''' full supervised learning '''\n",
    "    \n",
    "    model.train()\n",
    "    model.scale.freeze_bn()\n",
    "    model_d.train()\n",
    "\n",
    "    # train segmentation network\n",
    "    x, y = sample['image'], sample['class']\n",
    "\n",
    "    batch_len = len(x)\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    h = model(x)     # shape => (N, 8, H/8, W/8)\n",
    "    h = F.interpolate(h, size=(256, 320), mode='bilinear', align_corners=True)\n",
    "\n",
    "    h_ = h.detach()    # h_ is for calculating loss for discriminator\n",
    "    y_ = y.detach()    # y_is for the same purpose.  shape => (N, H, W)\n",
    "\n",
    "    d_out = model_d(h)    # shape => (N, 1, H/32, W/32)\n",
    "    d_out = F.interpolate(d_out, size=(256, 320), mode='bilinear', align_corners=True)    # shape => (N, 1, H, W)\n",
    "    d_out = d_out.squeeze()\n",
    "    \n",
    "    loss_ce = criterion_ce_full(h, y)\n",
    "    loss_adv = criterion_bce(d_out, ones[:batch_len])\n",
    "    loss_full = loss_ce + 0.01 * loss_adv\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_d.zero_grad()\n",
    "    loss_full.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # train discriminator\n",
    "    seg_out = model_d(h_)    # shape => (N, 1, H/32, W/32)\n",
    "    seg_out = F.interpolate(seg_out, size=(256, 320), mode='bilinear', align_corners=True)    # shape => (N, 1, H, W)\n",
    "    seg_out = seg_out.squeeze()\n",
    "    \n",
    "    y_ = one_hot(y_, 8, device)    # shape => (N, 8, H, W)\n",
    "    true_out = model_d(y_)    # shape => (N, 1, H/32, W/32)\n",
    "    true_out = F.interpolate(true_out, size=(256, 320), mode='bilinear', align_corners=True)    # shape => (N, 1, H, W)\n",
    "    true_out = true_out.squeeze()\n",
    "\n",
    "    loss_d_fake = criterion_bce(seg_out, zeros[:batch_len])\n",
    "    loss_d_real = criterion_bce(true_out, ones[:batch_len])\n",
    "    loss_d = loss_d_fake + loss_d_real\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_d.zero_grad()\n",
    "    loss_d.backward()\n",
    "    optimizer_d.step()\n",
    "    \n",
    "    return loss_full.item(), loss_d.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def semi_train(\n",
    "        model, model_d, sample, criterion_ce_semi, criterion_bce, \n",
    "        optimizer, optimizer_d, ones, zeros, device):\n",
    "\n",
    "    ''' semi supervised learning '''\n",
    "    \n",
    "    model.train()\n",
    "    model.scale.freeze_bn()\n",
    "    model_d.eval()\n",
    "\n",
    "    # train segmentation network\n",
    "    x = sample['image']\n",
    "    batch_len = len(x)\n",
    "    \n",
    "    x = x.to(device)\n",
    "    \n",
    "    h = model(x)     # shape => (N, 8, H/8, W/8)\n",
    "    h = F.interpolate(h, size=(256, 320), mode='bilinear', align_corners=True)\n",
    "\n",
    "    _, h_ = torch.max(h, dim=1)    # to calculate the crossentropy loss. shape => (N, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        d_out = model_d(h)    # shape => (N, 1, H/32, W/32)\n",
    "        d_out = F.interpolate(d_out, size=(256, 320), mode='bilinear', align_corners=True)    # shape => (N, 1, H, W)\n",
    "        d_out = d_out.squeeze()\n",
    "\n",
    "    loss_adv = criterion_bce(d_out, ones[:batch_len])\n",
    "\n",
    "\n",
    "    # if the pixel value of the output from discriminator is more than a threshold,\n",
    "    # its value is viewd as one from true label. Else, its value is ignored(value=255).\n",
    "    h_[d_out < 0.2] = 255\n",
    "\n",
    "    loss_ce = criterion_ce_semi(h, h_)\n",
    "    loss_semi = 0.001 * loss_adv + 0.1 * loss_ce\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_d.zero_grad()\n",
    "    loss_semi.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss_semi.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_model = './models/deeplabv2_resnet101_COCO_init.pth'\n",
    "class_weight_flag = True\n",
    "batch_size = 6\n",
    "num_workers = 4\n",
    "max_epoch = 1000\n",
    "learning_rate = 0.00025\n",
    "learning_rate_d = 0.0001\n",
    "n_classes = 8\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' DataLoader '''\n",
    "train_data_with_label = PartAffordanceDataset('train_with_label.csv',\n",
    "                                        transform=transforms.Compose([\n",
    "                                            CenterCrop(),\n",
    "                                            ToTensor(),\n",
    "                                            Normalize()\n",
    "                                        ]))\n",
    "\n",
    "train_data_without_label = PartAffordanceDatasetWithoutLabel('train_without_label.csv',\n",
    "                                        transform=transforms.Compose([\n",
    "                                            CenterCrop(),\n",
    "                                            ToTensor(),\n",
    "                                            Normalize()\n",
    "                                         ]))\n",
    "\n",
    "test_data = PartAffordanceDataset('test.csv',\n",
    "                            transform=transforms.Compose([\n",
    "                                CenterCrop(),\n",
    "                                ToTensor(),\n",
    "                                Normalize()\n",
    "                            ]))\n",
    "\n",
    "train_loader_with_label = DataLoader(train_data_with_label, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "train_loader_without_label = DataLoader(train_data_without_label, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "''' define model, optimizer, loss '''\n",
    "model = DeepLabV2_ResNet101_MSC(n_classes)\n",
    "model_d = Discriminator(n_classes)\n",
    "\n",
    "model.apply(init_weights)\n",
    "model_d.apply(init_weights)\n",
    "\n",
    "#state_dict = torch.load(pretrained_model)\n",
    "#model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "model.to(device)\n",
    "model_d.to(device)\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "                    params=[{\n",
    "                        \"params\": get_params(model, key=\"1x\"),\n",
    "                        \"lr\": learning_rate,\n",
    "                        \"weight_decay\": 5.0e-4,\n",
    "                            },\n",
    "                            {\n",
    "                        \"params\": get_params(model, key=\"10x\"),\n",
    "                        \"lr\": 10 * learning_rate,\n",
    "                        \"weight_decay\": 5.0e-4,\n",
    "                            },\n",
    "                            {\n",
    "                            \"params\": get_params(model, key=\"20x\"),\n",
    "                            \"lr\": 20 * learning_rate,\n",
    "                            \"weight_decay\": 0.0,\n",
    "                            }],\n",
    "                    momentum=0.9)\n",
    "\n",
    "\n",
    "\n",
    "optimizer_d = optim.Adam(model_d.parameters(), lr=learning_rate_d, betas=(0.9,0.99))\n",
    "\n",
    "if class_weight_flag:\n",
    "    class_weight = torch.tensor([0.0057, 0.4689, 1.0000, 1.2993, \n",
    "                                0.4240, 2.3702, 1.7317, 0.8149])    # refer to dataset.py\n",
    "    criterion_ce_full = nn.CrossEntropyLoss(weight=class_weight.to(device))\n",
    "else:\n",
    "    criterion_ce_full = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion_ce_semi = nn.CrossEntropyLoss(ignore_index=255)\n",
    "criterion_bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# supplementary constant for discriminator\n",
    "ones = torch.ones(batch_size, 256, 320).to(device)\n",
    "zeros = torch.zeros(batch_size, 256, 320).to(device)\n",
    "\n",
    "\n",
    "''' training '''\n",
    "\n",
    "losses_full = []\n",
    "losses_semi = []\n",
    "losses_d = []\n",
    "val_iou = []\n",
    "mean_iou = []\n",
    "best_iou = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "\n",
    "    epoch_loss_full = 0.0\n",
    "    epoch_loss_d = 0.0\n",
    "    epoch_loss_semi = 0.0\n",
    "\n",
    "    poly_lr_scheduler(\n",
    "        optimizer=optimizer,\n",
    "        init_lr=learning_rate,\n",
    "        iter=epoch - 1,\n",
    "        lr_decay_iter=10,\n",
    "        max_iter=max_epoch,\n",
    "        power=0.9,\n",
    "    )\n",
    "\n",
    "    poly_lr_scheduler_d(\n",
    "        optimizer=optimizer_d,\n",
    "        init_lr=learning_rate_d,\n",
    "        iter=epoch - 1,\n",
    "        lr_decay_iter=10,\n",
    "        max_iter=max_epoch,\n",
    "        power=0.9,\n",
    "    )\n",
    "\n",
    "\n",
    "    # only supervised learning\n",
    "    if epoch < 0:\n",
    "        for i, sample in tqdm.tqdm(enumerate(train_loader_with_label), \n",
    "                                   total=len(train_loader_with_label)):\n",
    "\n",
    "            loss_full, loss_d = full_train(\n",
    "                                    model, model_d, sample, criterion_ce_full, criterion_bce,\n",
    "                                    optimizer, optimizer_d, ones, zeros, device)\n",
    "            print(loss_full, loss_d)\n",
    "            epoch_loss_full += loss_full\n",
    "            epoch_loss_d += loss_d\n",
    "\n",
    "        losses_full.append(epoch_loss_full / i)   # mean loss over all samples\n",
    "        losses_d.append(epoch_loss_d / i)\n",
    "        losses_semi.append(0.0)\n",
    "\n",
    "\n",
    "    # semi-supervised learning\n",
    "    if epoch >= 0:\n",
    "        for i, (sample1, sample2) in tqdm.tqdm(enumerate(zip(train_loader_with_label, train_loader_without_label)), \n",
    "                                                total=len(train_loader_with_label)):\n",
    "\n",
    "            loss_full, loss_d = full_train(\n",
    "                                    model, model_d, sample1, criterion_ce_full, criterion_bce,\n",
    "                                    optimizer, optimizer_d, ones, zeros, device)\n",
    "            \n",
    "            epoch_loss_full += loss_full\n",
    "            epoch_loss_d += loss_d\n",
    "\n",
    "            loss_semi = semi_train(\n",
    "                                    model, model_d, sample2, criterion_ce_semi, criterion_bce,\n",
    "                                    optimizer, optimizer_d, ones, zeros, device)\n",
    "            \n",
    "            epoch_loss_semi += loss_semi\n",
    "\n",
    "        losses_full.append(epoch_loss_full / i)   # mean loss over all samples\n",
    "        losses_d.append(epoch_loss_d / i)\n",
    "        losses_semi.append(epoch_loss_semi / i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLabV2_ResNet101_MSC(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 3, 256, 320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/torch/lib/python3.5/site-packages/torch/nn/functional.py:1961: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    }
   ],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 33, 41])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, a = y.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 33, 41])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
